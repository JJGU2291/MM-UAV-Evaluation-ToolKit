# MM-UAV Evaluation Toolkit

#### MM-UAV benchmark page: [MM-UAV](https://xuefeng-zhu5.github.io/MM-UAV/)

#### MM-UAV baseline code: [MMA-SORT](https://github.com/JJGU2291/MM-UAV-Benchmark) 



the environment following the original `Readme_old.md`

use `pip3 -r install requirements.txt` to install all possible requirements.



**Step 1.** Place your tracking results under `data/trackers/MMMUAV/`, ensuring the following structure:

```
data/trackers/MMMUAV/
                |——your_experiment1/
                        |——track_results_rgb/
                                |——0001.txt
                                |——...
                        |——track_results_ir/
                                |——0001.txt
                                |——...
                |——your_experiment2/
                        |——track_results_rgb/
                                |——0001.txt
                                |——...
                        |——track_results_ir/
                                |——0001.txt
                                |——...
```

**Step 2.** Run one of the following scripts:

- `python scripts/run_mot_challenge_2modalities.py` (for basic evaluation)
- `python scripts/run_mot_challenge_2modalities_challenges.py` (for basic evaluation + per-attribute breakdown - takes longer to complete)

Evaluation results will be saved in corresponding subdirectories:

```
data/trackers/MMMUAV/
                |——your_experiment1/
                        |——track_results_rgb/
                        |——track_results_ir/
                        |——eval_results_rgb/      # Generated evaluation results
                        |——eval_results_ir/       # Generated evaluation results
                        ...
                |——your_experiment2/
                        |——track_results_rgb/
                        |——track_results_ir/
                        |——eval_results_rgb/      # Generated evaluation results
                        |——eval_results_ir/       # Generated evaluation results
                        ...
```

**Step 3.** Visualization

To compare and analyze the performance of different methods across various attributes:

Copy the result folders generated by `scripts/run_mot_challenge_2modalities_challenges.py` to `data/trackers/MMMUAV-for-plot/`. For example:

```
data/trackers/MMMUAV-for-plot/
                |——your_experiment1/
                        |——track_results_rgb/     # Optional
                        |——track_results_ir/      # Optional
                        |——eval_results_rgb/ 
                        |——eval_results_ir/ 
                        |——eval_results_ir_Lowillumination/
                        |——eval_results_rgb_Lowillumination/
                        ...
                        ...
                |——your_experiment2/
                        |——track_results_rgb/     # Optional
                        |——track_results_ir/      # Optional
                        |——eval_results_rgb/ 
                        |——eval_results_ir/ 
                        |——eval_results_ir_Lowillumination/
                        |——eval_results_rgb_Lowillumination/
                        ...
                        ...                      
```

Then run:

```
python scripts/plot_HOTA_2.py
```

This will generate a comprehensive comparison chart:

<p align="center"><img src="plot.png" width="50%" height="auto"/></p>

## Citation

```bibtex
@misc{xu2025trimodaldatasetbaselinetracking,
      title={A Tri-Modal Dataset and a Baseline System for Tracking Unmanned Aerial Vehicles}, 
      author={Tianyang Xu and Jinjie Gu and Xuefeng Zhu and XiaoJun Wu and Josef Kittler},
      year={2025},
      eprint={2511.18344},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2511.18344}, 
}
```


## Acknowledgement

A significant portion of the code is borrowed from [TrackEval](https://github.com/JonathonLuiten/TrackEval) We thank the authors for their excellent work!